{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "DATA_PATH = '폴더경로/DATA/' # TODO 데이터 경로 설정\n",
    "train_data = pd.read_csv(DATA_PATH+'ratings_train.txt', header = 0, delimiter='\\t', quoting=3)\n",
    "\n",
    "train_data['document'][:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 함수 만들기\n",
    "def preprocessing(review, okt, remove_stopwords = False, stop_words =[]):\n",
    "  #함수인자설명\n",
    "  # review: 전처리할 텍스트\n",
    "  # okt: okt객체를 반복적으로 생성하지 않고 미리 생성 후 인자로 받음\n",
    "  # remove_stopword: 불용어를 제거할지 여부 선택. 기본값 False\n",
    "  # stop_words: 불용어 사전은 사용자가 직접 입력, 기본값 빈 리스트\n",
    "\n",
    "  # 1. 한글 및 공백 제외한 문자 모두 제거\n",
    "  review_text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]','',review)\n",
    "  \n",
    "  #2. okt 객체를 활용하여 형태소 단어로 나눔\n",
    "  word_review = okt.morphs(review_text,stem=True)\n",
    "\n",
    "  if remove_stopwords:\n",
    "    #3. 불용어 제거(선택)\n",
    "    word_review = [token for token in word_review if not token in stop_words]\n",
    "  return word_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 텍스트 전처리\n",
    "stop_words = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한']\n",
    "okt = Okt()\n",
    "clean_train_review = []\n",
    "\n",
    "for review in train_data['document']:\n",
    "  # 리뷰가 문자열인 경우만 전처리 진행\n",
    "  if type(review) == str:\n",
    "    clean_train_review.append(preprocessing(review,okt,remove_stopwords=True,stop_words= stop_words))\n",
    "  else:\n",
    "    clean_train_review.append([]) #str이 아닌 행은 빈칸으로 놔두기\n",
    "\n",
    "clean_train_review[:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 리뷰도 동일하게 전처리\n",
    "test_data = pd.read_csv(DATA_PATH + 'ratings_test.txt', header = 0, delimiter='\\t', quoting=3)\n",
    "\n",
    "clean_test_review = []\n",
    "for review in test_data['document']:\n",
    "  if type(review) == str:\n",
    "    clean_test_review.append(preprocessing(review, okt, remove_stopwords=True, stop_words=stop_words))\n",
    "  else:\n",
    "    clean_test_review.append([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 벡터 변환 후 일정 길이 넘어가거나 모자라는 리뷰 패딩처리\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_review)\n",
    "train_sequences = tokenizer.texts_to_sequences(clean_train_review)\n",
    "test_sequences = tokenizer.texts_to_sequences(clean_test_review)\n",
    "\n",
    "word_vocab = tokenizer.word_index #단어사전형태\n",
    "MAX_SEQUENCE_LENGTH = 8 #문장 최대 길이\n",
    "\n",
    "#학습 데이터\n",
    "train_inputs = pad_sequences(train_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "\n",
    "#학습 데이터 라벨 벡터화\n",
    "train_labels = np.array(train_data['label'])\n",
    "\n",
    "#평가 데이터 \n",
    "test_inputs = pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "#평가 데이터 라벨 벡터화\n",
    "test_labels = np.array(test_data['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PATH  = '전처리 된 데이터를 저장할 경로입력' # TODO 경로지정\n",
    "DATA_PATH = 'CLEAN_DATA/' #.npy파일 저장 경로지정\n",
    "TRAIN_INPUT_DATA = 'nsmc_train_input.npy'\n",
    "TRAIN_LABEL_DATA = 'nsmc_train_label.npy'\n",
    "TEST_INPUT_DATA = 'nsmc_test_input.npy'\n",
    "TEST_LABEL_DATA = 'nsmc_test_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "data_configs={}\n",
    "data_configs['vocab'] = word_vocab\n",
    "data_configs['vocab_size'] = len(word_vocab) + 1\n",
    "\n",
    "#전처리한 데이터들 파일로저장\n",
    "import os\n",
    "# CLEAN_DATA 폴더가 없을 시 폴더 생성\n",
    "if not os.path.exists(DEFAULT_PATH + DATA_PATH):\n",
    "  os.makedirs(DEFAULT_PATH+DATA_PATH)\n",
    "\n",
    "#전처리 학습데이터 넘파이로 저장\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TRAIN_INPUT_DATA,'wb'),train_inputs)\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TRAIN_LABEL_DATA,'wb'),train_labels)\n",
    "#전처리 테스트데이터 넘파이로 저장\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TEST_INPUT_DATA,'wb'),test_inputs)\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TEST_LABEL_DATA,'wb'),test_labels)\n",
    "\n",
    "#데이터 사전 json으로 저장\n",
    "json.dump(data_configs,open(DEFAULT_PATH + DATA_PATH + DATA_CONFIGS,'w'),ensure_ascii=False)\n",
    "\n",
    "# tokenizer 인덱스 값 저장\n",
    "import pickle\n",
    "with open(DEFAULT_PATH + DATA_PATH + 'tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_vocab, handle)\n",
    "[출처] [NLP] 한국어 감정분류(2) - 학습을 위한 데이터 전처리 (2022 G-BIG 지식카페) | 작성자 기술코치 임원화"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
